Got it. I’ll generate a fully detailed and implementation-ready to-do list for the Compliance & GA Readiness tasks you mentioned, strictly aligned with the Version 5 blueprint. This will include the GDPR JSON/CSV export stub, the Right-to-be-Forgotten cascade SQL, the RPO/RTO documentation steps, and the final security pen-test checklist — all spelled out with no ambiguity.

I’ll update you once it’s ready.


# Compliance & GA Readiness To-Do List (Blueprint v5)

## 1. GDPR JSON/CSV Export Stub

* [ ] **Implement a secure data export endpoint:** Create an admin-only route (Supabase Edge Function or Next.js API) that compiles **all user data within the tenant’s schema** (e.g. leads, pages, forms, etc.) into a machine-readable export. The endpoint should generate both JSON and CSV representations of the data (e.g. a JSON file containing all tables’ data, and CSV files per table or equivalent).

  * Ensure this export covers **every table in the tenant’s schema** and captures all relevant records and fields.
  * The output should be provided as a downloadable file or secure link (e.g. a generated file download) for the tenant.
* [ ] **Enforce strict access control:** Restrict this export functionality to authorized users only (e.g. tenant owners or platform admins per RBAC rules). The function/route must derive the target schema from the caller’s context (JWT claims or session) and **only export that tenant’s data**, guaranteeing no cross-tenant data is included. All requests must be authenticated and validated against the tenant’s ID to respect schema boundaries.
* [ ] **Integrate with admin UI (if applicable):** If a UI trigger is needed (for GA user-initiated export), add an **“Export Data”** action in the tenant’s admin dashboard that invokes this endpoint. Ensure the UI clearly indicates the export format (JSON/CSV) and that the operation may be resource-intensive (consider asynchronous processing if data volume is large). *(Note: In MVP this was handled offline via PDF, but GA requires user-accessible export).*
* [ ] **Test the export output:** After implementation, run the export for a sample tenant and verify the JSON and CSV files are correct: all tables present, data accurately captured, and no data from other tenants. Also verify that unauthorized users (e.g. a regular member or a coach without proper rights) cannot access this endpoint (should be forbidden).

## 2. Right-to-Be-Forgotten Cascade SQL

* [ ] **Implement cascade-delete function:** Develop a **secure Supabase database function or SQL stored procedure** to **delete all personal data for a given end-user** within a tenant’s schema, fulfilling GDPR “Right to be Forgotten” requirements. This function should accept an identifier for the user (e.g. email address or lead ID) and then **cascade through all relevant tables** in that tenant schema to remove any records pertaining to that user.

  * Include deletions in tables such as `<schema>.leads` (captured lead info) and any other tables where the user’s data might reside (e.g. form submissions, related records). Use foreign key cascades if defined; otherwise, explicitly delete child records (for example, entries in a `<schema>.forms_submissions` table, etc.) before removing the main profile in `<schema>.leads`. Ensure no orphaned references remain.
  * The deletion should be performed **within a single transaction** to guarantee that either all of the user’s data is removed across the schema or nothing is (for consistency). Add any necessary safety checks (for instance, verifying the email/ID exists in the schema) and log the action if required for audit.
* [ ] **Secure the deletion operation:** Restrict execution of this function to privileged roles. It should **only be invocable by an admin or the tenant owner**, not by general end-users directly. If exposed via an API or Edge Function, that endpoint must enforce that the requester has authority to delete the specified user’s data (e.g. a coach or owner of that account, not an external party).
* [ ] **Integrate request handling:** Provide a way to trigger this deletion when a GDPR erasure request is received. For example, introduce an admin panel action like “Delete User Data” where an admin can input an email/ID and initiate the purge, or accept a support ticket reference. This will call the above SQL function for the tenant’s schema.
* [ ] **Thorough testing of cascade delete:** Populate a test tenant schema with a mock user’s data (create leads, form entries, etc. for a sample email), run the “forget” procedure, and confirm that **all related records are removed**. Verify that data for other users remains intact and that no cross-tenant deletion is possible (the function should not affect any schema except the one it’s run in). Also ensure that attempting to delete a non-existent user fails gracefully with no side effects.

## 3. Document RPO ≤ 30 min / RTO ≤ 2 h (README & SLA)

* [ ] **Update README with backup & recovery policy:** In the project’s main README, add a **“Backup and Recovery”** section that explicitly states the platform’s recovery objectives. Include wording that **Recovery Point Objective (RPO) is ≤ 30 minutes** and **Recovery Time Objective (RTO) is ≤ 2 hours**, as per the blueprint’s specifications. Describe at a high level how this is achieved (e.g. *“We utilize Supabase’s managed continuous backups and point-in-time recovery to ensure that no more than 30 minutes of data would be lost in a disaster, and full restoration of service can be accomplished within 2 hours”*). Make sure to mention that this supports a 99.9% uptime commitment.
* [ ] **Include RPO/RTO in the SLA documentation:** If an SLA or security appendix is provided to customers (e.g. in documentation or an agreement), incorporate the same **RPO ≤ 30 min and RTO ≤ 2 hr** guarantees there. Write this in a formal manner, for example: *“**Data Durability and Recovery:** The Ozza platform maintains an RPO of 30 minutes and an RTO of 2 hours. In the event of a catastrophic failure, at most 30 minutes of recent data might not be recoverable, and full service restoration is guaranteed within 2 hours. This is enabled by continuous backups and point-in-time recovery features of our managed database.”* Ensure this aligns with the 99.9% uptime SLA promise.
* [ ] **Verify backup configuration:** Confirm that Supabase **Point-in-Time Recovery (PITR)** or continuous backup is enabled on the production database, as this is the mechanism to meet the RPO/RTO. Document in an internal ops note if needed: how to perform a point-in-time restore and the procedures to recover within 2 hours. (This is for completeness, though not in the README, it backs up the claims made in documentation.)
* [ ] **Cross-reference in changelog:** If there is a GA launch changelog or version notes, note that the documentation now includes these RPO/RTO commitments (fulfilling the GA readiness criteria in the blueprint). This ensures transparency that these objectives are part of the platform’s guarantees.

## 4. Final Security Pen-Test Checklist

Before finalizing GA release, perform a comprehensive security review as specified. The following checklist covers all critical areas from the blueprint’s security spec:

1. **Tenant Isolation Validation:**

   * [ ] **Cross-tenant data access test:** Create two or more test tenants (e.g. Account A and Account B), each with their own data. While authenticated as a user from Account A, attempt to retrieve or manipulate data belonging to Account B (for example, by crafting API requests with IDs from Account B or by attempting direct SQL queries if possible). **Confirm that all such access is blocked** – no data from Account B should ever be returned to Account A. This validates that physical schema separation and Row-Level Security are effectively preventing any cross-tenant data leakage.
   * [ ] **Coach access scope:** If the platform has a **coach role** that can view multiple tenant accounts (agencies), test that a coach logged in under their account can only access the data of the specific client agencies they are permitted to see. Verify that the coach cannot see any data for agencies not assigned to them. Additionally, check that any **personally identifiable information (PII)** in the data a coach views is properly **masked** according to the privacy rules (e.g. email addresses or phone numbers of leads should appear partially anonymized). This ensures that even authorized cross-tenant views are limited and privacy-preserving.
   * *(*Expected outcome:* RLS policies and schema-based restrictions should make it impossible for a user or coach to access another tenant’s records. Coaches should only see allowed data, and sensitive fields should be obfuscated as defined.)*

2. **Supabase RLS Enforcement Tests:**

   * [ ] **Policy completeness check:** Review the database schemas (especially the `public` schema and any shared tables) to ensure that **Row-Level Security is enabled on every table that holds tenant-specific data**. Each RLS policy should restrict access by matching the row’s `account_id` (or schema) to the user’s `account_id` claim in their JWT. No table that could expose multiple tenants’ data should be left without an RLS policy.
   * [ ] **Attempted bypass scenarios:** Try to intentionally bypass RLS in a controlled manner. For example, use a Supabase service-role key in a test environment to simulate a query that a malicious client might attempt (since service role bypasses RLS), and ensure that such queries are never exposed to regular users in the application. Also verify that the application **never uses a superuser or service role on the client side** and that no configuration accidentally disables RLS. The blueprint mandates no “shortcuts” such as using the service role in client code or turning off RLS even temporarily.
   * [ ] **Unauthorized data access attempts:** As an authenticated user, attempt to query data via the Supabase client or API using various methods (e.g. altering the request parameters or using a secondary client) to see if any data not belonging to your account is returned. All such attempts should return empty results or access errors. This double-checks that the RLS policies are not only in place but effectively enforced in practice.
   * *(*Expected outcome:* RLS is active and correctly configured on all multi-tenant tables. Any query made with a normal user context will automatically be filtered to that user’s tenant. Attempts to disable or circumvent RLS are prevented by design – the client uses only restricted roles, and the system enforces least privilege.)*

3. **JWT Claim Integrity Checks:**

   * [ ] **Server-side authorization logic:** Verify that all server-side endpoints (Next.js API routes and Supabase Edge Functions) are validating the user’s JWT claims against the requested resources. For every API that takes an `account_id` or resource ID as input, the implementation should ensure the ID belongs to the current user’s tenant before proceeding. This means the backend is cross-checking JWT’s `account_id` against the target data’s `account_id` in queries or using context-aware database calls. Review the code to confirm these checks exist universally (especially for any endpoints that perform updates or reads on behalf of the user).
   * [ ] **Tampering test:** Simulate a malicious attempt by altering an API request to use an ID from another tenant (for instance, calling an URL or GraphQL query with a different `account_id` or object ID that the user does not own). The expected result is that the server returns an error or no data (e.g. **HTTP 403 Forbidden** or simply an empty result set) due to the combined protections of JWT claim verification and RLS. This tests the “dual defense” described in the blueprint: even if RLS were misconfigured, the application logic would catch the mismatch (and vice versa).
   * [ ] **JWT issuance review:** Confirm that the custom JWT claims (account\_id and role) are being correctly set by the Supabase JWT trigger function upon login, and that these claims cannot be altered by the client. (While this is largely handled by Supabase, it’s important to ensure the claims reflect the user’s true permissions. Any discrepancy could allow unauthorized access.)
   * *(*Expected outcome:* The system should never allow a user to access another tenant’s data by simply guessing or altering identifiers. The JWT’s account context is consistently checked on every privileged operation. All observed behavior should align with the rule that a user’s `account_id` in the token dictates the only data they can access.)*

4. **Edge Function Access Hardening:**

   * [ ] **Authentication and input validation:** Audit all Supabase Edge Functions deployed (e.g. the Stripe webhook, any export/delete functions, etc.) to ensure they enforce proper authentication for their use-cases. For functions meant to be invoked by end-users (such as a future GDPR export or delete function), verify that they **require a valid JWT** and that the JWT’s claims (tenant and role) are checked against the operation (for example, only an owner or admin’s token can trigger a full data export or deletion). For public-facing functions (like webhooks that cannot have a JWT), ensure that an alternative secret verification is in place (e.g. the Stripe webhook function uses the Stripe signature header to validate authenticity).
   * [ ] **Scope of operations:** Review the database queries and operations inside each Edge Function to confirm they are **narrowly scoped to their intended purpose**. Since these functions run with elevated privileges (service role, bypassing RLS), they must **manually enforce the tenant boundaries**. For example, an export function should construct queries to only read from the specific tenant’s schema (using the schema name or account\_id from the JWT), and a deletion function should only delete records within the target tenant. Ensure no function can perform actions beyond what it’s designed for (no broad `DELETE *` without a tenant filter, no access to other tenants’ schemas, etc.). Also double-check that environment secrets (like the Stripe webhook secret) are properly used and not exposed.
   * [ ] **Pen-test simulation:** Try calling the Edge Functions in ways they weren’t intended – e.g., without a JWT, with an invalid JWT, or with manipulated inputs (such as specifying another tenant’s ID in the payload). All such requests should be rejected by the function’s logic. The function should respond with an error or simply do nothing in these cases, and never leak data.
   * *(*Expected outcome:* Each Edge Function should act as a fortress around its specific functionality. Only legitimate, authenticated requests should succeed, and even then, the function’s code limits any database operations to the correct tenant scope. There should be no path for an attacker to exploit an Edge Function to gain broader access.)*

5. **Coach Access Audits:**

   * [ ] **Audit log implementation:** Ensure that an **audit logging mechanism** is in place to record every time a coach (or other privileged user) accesses data from a tenant that is not their own (i.e., any cross-tenant data view). According to the blueprint, the system should be logging the **who, what, and when** of such access. Concretely, verify that the `public.audit_log` (or equivalent) table is recording entries with details such as: the ID of the coach user, the ID of the tenant/agency whose data was accessed, the action or route used, and a **checksum or hash of the data returned** (to provide an evidence trail of exactly what was seen). Logging should occur automatically whenever a coach uses any impersonation or proxy view feature to read another tenant’s information.
   * [ ] **Audit log verification test:** Perform an action as a coach that triggers cross-tenant data access (for example, use a “coach view” feature to view a client agency’s dashboard or reports). Then query the audit log to confirm that a new entry was created for this event. Check that the entry contains all expected fields (coach’s user ID, target account ID, timestamp, and some representation of the data viewed). Also ensure that these log records are **protected by RLS** or otherwise not exposed to unauthorized users – only platform admins or the relevant tenant (if appropriate) should be able to see them.
   * [ ] **Review retention and monitoring:** While not explicitly in the blueprint, consider how these audit logs are retained and monitored. For GA readiness, ensure there is a plan for reviewing these logs for any anomalies during penetration testing or security reviews (especially since this is crucial for SOC 2 compliance).
   * *(*Expected outcome:* Every privileged data access by coaches is transparently logged. In a pen-test, the testers should find that any attempt to use coach privileges is traceable in the system, which deters misuse and provides accountability. No gaps exist where a coach could access data without leaving a trail.)*

6. **Admin “Sudo” Mode Validations:**

   * [ ] **Two-factor auth requirement:** Verify that the **Platform Admin emergency access (sudo mode)** can only be activated with the proper secondary verification. As per spec, the platform admin (founder/lead engineer) must perform a 2FA challenge before gaining access to a tenant’s schema. Check that this is implemented – for example, if the admin uses a CLI command or a special route to assume a tenant identity, the system should prompt for a one-time code or require a secure 2FA token confirmation. Without this step, access should be denied.
   * [ ] **Reason logging:** Confirm that whenever sudo mode is used, the admin is **required to provide a reason or ticket ID** for the access. The system should record this reason along with the event. Check the audit log or a dedicated admin access log for entries whenever the admin enters or exits a tenant schema in this manner. Each log should include the admin’s ID, the target tenant account, timestamp, and the provided reason for access. This ensures strict oversight of admin actions.
   * [ ] **Functional test of sudo access:** Perform a controlled test of the sudo mode procedure: use the CLI or admin interface to impersonate a tenant after fulfilling 2FA and reason entry. Verify that the admin can indeed **access the tenant’s data** after elevation (confirming the mechanism works), and then revoke that access. Ensure that during this period, the admin’s actions are constrained to the target schema and that when the session ends, the admin’s privileges return to normal. All steps of this process should have been logged.
   * [ ] **No untracked admin access:** Double-check that there are no alternative backdoors or shortcuts for admins to get into tenant data without going through this 2FA + logging process. The blueprint explicitly disallows any hidden superuser access; everything must be auditable and go through the approved procedure. If the penetration testers try to find a way to escalate privileges, they should only find this officially sanctioned path, which is tightly controlled.
   * *(*Expected outcome:* The emergency admin access is gated behind additional security and leaves an audit trail. Penetration testing should validate that an admin cannot secretly access tenant data — any such access is protected by strong controls and visible in logs. This satisfies the GA security requirement that even administrators are subject to accountability.)*

Each item in this checklist should be verified and checked off prior to GA. All tasks above align strictly with the **Ozza Platform Implementation Blueprint (v5)** specifications for compliance and security readiness, ensuring the platform meets GDPR requirements and is prepared for a thorough security audit. Each measure (data export, data deletion, documented recovery objectives, and security validations) is implemented exactly as defined in the blueprint to eliminate ambiguity and guarantee compliance.
