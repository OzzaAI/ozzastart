Thanks for confirming it's based on the blueprint. I’ll create a detailed and unambiguous to-do list for implementing Observability and AI-Usage Metering, including ai\_usage\_events, the nightly roll-up job, and error monitoring via Grafana/Loki and Supabase logs.

I’ll break this down into actionable steps, each with precise instructions so there’s no ambiguity or creative interpretation required.
I’ll let you know when it's ready.


# Observability & AI‑Usage Metering Implementation To-Do List

1. **Add `public.ai_usage_events` table for AI usage logging (with RLS and server-side enforcement):**

   * **Schema Design:** Create a new table in the shared `public` schema named `ai_usage_events` (to log AI feature usage for each tenant). Include columns: `id` (primary key, e.g. UUID or bigserial), `account_id` (UUID, references the tenant in `public.accounts`), `event_type` (text, e.g. type of AI operation), `tokens_used` (integer count of tokens or credits consumed), and `created_at` (timestamp of the event, default to now). This schema follows the naming conventions (lowercase, snake\_case) and multi-tenant structure outlined in the blueprint.
   * **Row-Level Security:** Enable RLS on `public.ai_usage_events` so that each tenant’s users can only access their own usage records. Define a policy such that a row’s `account_id` must match the user’s JWT `account_id` claim for SELECT (allowing a tenant to view *their* usage). Do **not** allow regular users to INSERT/UPDATE/DELETE on this table (no such RLS policy), since usage events should only be added by server-side processes. The Supabase service role (or a SECURITY DEFINER function) will bypass RLS to insert events. This ensures tenants cannot forge or alter usage data of others, preserving strict data isolation as required.
   * **Plan Quota Integration:** Ensure the `public.plans` table has a field for included AI usage credits (e.g. `included_ai_credits` for monthly token allowance) if not already present. Update seed data so each plan has an appropriate allowance (e.g. Free = 0, Pro = X tokens/month, etc.), reflecting the new **AI Usage & Cost Controls** introduced in Blueprint v5. This ties the usage logging to each tenant’s subscription plan limits. Also verify a feature flag exists for AI usage (e.g. `AI_CONTENT_ASSISTANT` in `public.features`), and that on account creation or plan upgrade, an entry in `public.account_features` enables this flag for accounts on plans that include AI credits. This follows the blueprint practice of gating features per plan via flags and ensures the front-end only exposes AI features to eligible tenants. **RLS** on `account_features` remains in effect (each account sees only its flags).
   * **Server-Side Usage Enforcement:** Implement a robust server-side mechanism to record usage events and enforce plan limits. For example, write a Postgres function `log_ai_usage(account_id, event_type, tokens_used)` with SECURITY DEFINER rights, or a BEFORE INSERT trigger on `ai_usage_events`, that will **validate the tenant’s remaining credits before insertion**. This function/trigger should: (1) look up the account’s plan (`accounts.plan_id` → `plans.included_ai_credits`), (2) calculate the total tokens used by that account in the current billing period (e.g. sum of `tokens_used` for that `account_id` where `created_at` is in the current month), and (3) if adding the new usage would exceed the plan’s `included_ai_credits`, then **abort** the operation (e.g. raise an exception) to prevent overage. This implements the blueprint’s mandate to throttle AI costs within each tenant’s plan limits (stop further AI usage once the allowance is exhausted). If the new usage is within the limit, the function inserts the record into `ai_usage_events`. By performing the check in the database, we enforce quotas even if a client bypasses the UI – aligning with the blueprint’s multi-layer enforcement of plan rules (UI, API, and DB).
   * **Edge Function Integration:** Update the relevant Supabase Edge Functions (the serverless functions that execute AI actions) to use the above logging mechanism. After a successful AI operation (e.g. an OpenAI API call), the Edge Function should call the `log_ai_usage` function (or insert into `ai_usage_events` directly) using the service role, passing the current tenant’s `account_id`, the event type (e.g. `"content_generation"`), and the tokens consumed. Because the Edge Function runs with elevated privileges, it will bypass RLS and trigger the server-side check. If the function/trigger raises a limit error (meaning the tenant has exhausted their AI credits), handle that in the Edge Function by returning a clear response (e.g. a 402 Payment Required or 429 Too Many Requests with a message that the quota is exceeded). This ensures **server-side throttling** is in place: even if the frontend missed a check, the backend will not execute or will block over-limit usage. All usage events (successful or even attempted if we log those) are thus recorded, creating an audit trail of AI usage per tenant. Confirm that these Edge Functions are deployed behind a feature flag or plan check (only invoke them if the tenant’s `AI_CONTENT_ASSISTANT` feature is enabled), consistent with feature flag practices.

2. **Implement a nightly AI usage roll-up job for aggregation and quota tracking:**

   * **Roll-up Table/View Design:** Create a derived table or materialized view (e.g. `public.ai_usage_rollups`) that aggregates AI usage per tenant per day. The structure should include `account_id`, `usage_date` (date dimension, e.g. UTC date), and `total_tokens_used` for that account on that date. This pre-computed summary aligns with the blueprint’s guidance to use **materialized views for aggregate metrics** across tenants to avoid expensive real-time queries. If using a table, define a primary key on (`account_id`, `usage_date`) and indices as needed for fast lookups by account or date. Define RLS on this roll-up as well so tenants can only see their own aggregated usage (just like the events table). (Coaches or admins with cross-tenant visibility can be handled via specialized roles or by bypassing RLS when querying this table, per the blueprint’s approach to cross-tenant access logging/auditing.)
   * **Aggregation Logic:** Write a SQL query or stored procedure to populate the roll-up. For example: *for each account*, sum the `tokens_used` from `ai_usage_events` where `created_at` falls in a given day. You can implement this as a materialized view definition:

     ```sql
     CREATE MATERIALIZED VIEW public.ai_usage_rollups AS
     SELECT account_id, 
            date_trunc('day', created_at)::date AS usage_date,
            SUM(tokens_used)::bigint AS total_tokens_used
     FROM public.ai_usage_events
     GROUP BY account_id, date_trunc('day', created_at);
     ```

     If using a table instead (for easier incremental updates), you might create a temp table or CTE for yesterday’s data and upsert into `ai_usage_rollups`. The goal is to refresh this data **nightly** (after each day’s end) so that the system has up-to-date usage totals per day for every tenant. This approach follows the blueprint’s recommendation of **pre-computing multi-tenant reports** rather than scanning many schemas or rows on the fly.
   * **Scheduled Nightly Job:** Set up an automated job to perform the roll-up refresh once per day (e.g. at midnight). If Supabase offers a scheduling mechanism (like pg\_cron or a Supabase Scheduler for Edge Functions), use it to call a function that refreshes the materialized view or populates the table. If such a scheduler is not available, configure an external cron job or cloud scheduler to invoke a Supabase Edge Function at the desired time (the Edge Function can then execute a `REFRESH MATERIALIZED VIEW` or run the aggregation SQL using the service role). Ensure the job runs with appropriate privileges (service role) so it can read all `ai_usage_events` across tenants and write the rollups. Also, consider time zone and billing-cycle alignment: if plan resets are monthly, ensure the roll-up can also be aggregated by month if needed for monthly quota checks (you might later add a `usage_month` aggregation). The external scheduler’s credentials (like a service API key or a secure webhook token) should be stored securely in the environment configuration, not hard-coded.
   * **Use Rollups for Quota & Analytics:** Modify the quota enforcement and analytics logic to leverage the roll-up data. For example, when checking if an account has remaining AI credits, sum the current month’s entries in `ai_usage_rollups` (which is much faster than scanning all events) and compare to `plans.included_ai_credits`. This optimization is in line with blueprint advice to keep performance efficient as tenants grow. The rollup table can also feed **analytics dashboards**: e.g. a “Coach Scorecard” could query each managed account’s daily or monthly usage from `ai_usage_rollups` in one go, rather than hitting each tenant’s schema or the raw events (the blueprint explicitly suggests storing derived data in `public` for cross-tenant reporting like this). By using pre-aggregated usage metrics, coaches and admins get fast access to usage trends (how much AI each agency is using) without compromising isolation or performance. *(Note: If any in-app **usage alert** feature is planned – for instance, warning an agency at 80% of their quota – this roll-up data can help trigger those alerts efficiently. We ensure the data needed for such features is readily available.)*

3. **Set up observability dashboards and alerts for AI usage and system health:**

   * **Logging and Dashboarding:** Establish monitoring for both AI usage and overall system errors, leveraging Supabase’s tools and external solutions as needed. For the **Stripe billing webhooks** (which update plans and statuses), use Stripe’s dashboard to monitor webhook delivery status (failed vs. successful attempts). Additionally, instrument the Stripe webhook Supabase Function to log any errors or retries to a centralized log (Supabase Logs or an external log aggregator like Grafana Loki). Include context like the event type and Stripe’s `Retry-Num` header if available, so you can see if a webhook is being retried multiple times. For **Supabase Edge Functions** in general (including the AI usage functions), use Supabase’s built-in monitoring and logs to track invocation counts, execution duration, and error occurrences. Setting up a Grafana dashboard with data from these logs (or using Supabase’s own monitoring UI) can give a live view of function performance and failure rates. Integrate an error tracking tool (the blueprint mentions possibly using Sentry for the Next.js app) to catch front-end and API route errors as well – this ensures that issues in usage reporting or billing logic surface quickly if they affect the user experience.
   * **Performance Metrics Monitoring:** Monitor database and infrastructure metrics critical to multi-tenant performance. In particular, track **database connection usage** over time – Supabase’s analytics or a custom Grafana panel should show how many connections are in use, helping to ensure you don’t approach the limit as more tenant schemas are added. Watch for any spike in connections or long-running queries (e.g., if an AI usage query or roll-up is slow, it might show up in slow query logs). Also monitor **error logs and CPU/memory usage** on the database and edge function side. Supabase’s platform provides some of this, but you can augment with custom Prometheus/Grafana metrics if needed. Keep an eye on **schema count growth** (number of tenant schemas or rows in `public.accounts`); the blueprint notes testing up to \~1,000 schemas and the need to plan if approaching “five figures”, so having a metric for total tenants is useful. This can simply be a nightly count of `accounts` or schemas, displayed on a dashboard to observe growth trends.
   * **Alerts for Anomalies and Limits:** Configure automated alerts to notify the team of potential issues or breaches of limits. For example, set an alert if the Stripe webhook function fails or throws errors X times within a short period (indicating a possible issue with processing billing events promptly). Similarly, alert on elevated error rates in the AI-related Edge Functions – e.g., if the `log_ai_usage` function starts throwing quota exceptions very frequently or any unhandled errors occur, as that could indicate either abuse or a bug. Set up **database health alerts**: if DB connections exceed, say, 80% of the max, or if CPU usage stays high for an extended period, have the monitoring system send a Slack message or email to admins. This proactive alerting aligns with the blueprint’s emphasis on responding quickly to issues beyond just automated tests. Additionally, implement an alert for **unusual AI usage spikes**: if a single tenant’s usage suddenly jumps well above their plan allowance or the normal usage pattern (which could incur unexpected cost), flag it. This supports cost control by ensuring the team is aware of any tenant that might be abusing AI features or racking up cost – a safety net alongside the hard quota enforcement. The blueprint’s cost control guidelines imply keeping an eye on such patterns (e.g. to decide on overage charges or upsells).
   * **Configuration & Hygiene:** Document all new monitoring and metering configuration. Add any required API keys or endpoints (for Grafana, Loki, Sentry, Slack webhooks, etc.) to the **environment configuration** or a secure settings store – *never* hard-code them. Follow the platform’s feature-flag hygiene practices for any new toggles introduced (for example, if an **AI usage monitoring** feature flag or a **scheduler activation** flag is used, ensure it’s documented and will be reviewed periodically). Finally, include the observability setup in your regular maintenance: verify logs and dashboards are capturing data correctly in staging before relying on them in production, and set aside time (perhaps in each sprint) to review these metrics. This disciplined approach to observability will help catch issues early and is part of the platform’s commitment to stability and cost-effectiveness as it scales.

Each of these steps should be implemented **in line with the blueprint v5 instructions**, using the same naming conventions, security isolation, and multi-layer enforcement philosophy. By completing this to-do list, the Ozza platform will gain fine-grained AI usage metering (with per-tenant quotas and audit trails) and a robust observability framework to monitor system health and usage patterns – all critical for a reliable, scalable multi-tenant SaaS launch.

**Sources:** The implementation details above are derived from the Ozza Platform Implementation Blueprint (v5), ensuring faithful adherence to the prescribed architecture and practices.
