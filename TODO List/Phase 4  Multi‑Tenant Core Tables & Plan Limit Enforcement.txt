Thanks! I’ll now generate a complete and unambiguous TO DO list for implementing tenant schema bootstrapping and plan-limit enforcement triggers (max users, max sites) based strictly on the Website Blueprint v5.

This will include migration steps, schema and trigger logic, permissions, and CLI/testing hooks so there’s no ambiguity for AI or engineers. I’ll notify you once it’s ready.


# Implementation Checklist: Multi‑Tenant Core Tables & Plan Limit Enforcement

## Phase 1: Migration/CLI Setup – Add Core Tables to All Tenant Schemas

* **Enumerate Tenant Schemas:** Query the central accounts registry to get all tenant schema names (e.g. `SELECT schema_name FROM public.accounts`). Exclude non-tenant schemas (`public`, `auth`, etc.).
* **Define Required Core Tables:** List the standard tables each tenant schema must have (per Blueprint Section 3) – e.g. `sites`, `pages`, `forms`, `leads` (and any others like `media`, `analytics` if introduced). Each table should use **lowercase snake\_case** names and columns to meet naming conventions.
* **Schema Loop Implementation:** Implement a **migration or CLI command** (e.g. `ozza-cli refresh-core-tables`) that iterates over each tenant schema and ensures these core tables exist:

  * *Connection & Privileges:* Run the operation with a privileged role (the Supabase service role or migration role) that has rights to create tables in all schemas. This avoids permission issues when creating cross-schema objects.
  * *Table Creation:* For each `<schema>` in the list and for each required table:

    * **Check Existence:** If the table does not exist in that schema, create it with the exact schema as per the blueprint. Include all expected columns and constraints so it conforms to the **data model** in Blueprint v5 (e.g. primary key `id` (UUID or serial PK), foreign keys within the schema, etc.). For example, `<schema>.sites` gets `id` (PK), `name`, optional `subdomain`/`domain` field, `status`, etc. and `<schema>.pages` gets `id` (PK), `site_id` (FK to sites.id), `title`, `content` JSON/text, etc.. Use **additive changes only** (no dropping or renaming) in line with the blueprint’s rule.
    * **Account Reference Column:** If not already present, add an `account_id UUID` column to each core table (for clarity and indexing) and default it to that tenant’s UUID. This ensures each row can explicitly reference its owning account, aligning with the blueprint’s consistency guidelines (no cross-schema foreign keys, but an explicit account reference for joins). After adding, backfill existing rows with the account’s ID (lookup by matching `public.accounts.schema_name`).
    * **Intra-Schema Constraints:** Add any necessary **foreign keys** within the tenant schema (e.g. make `<schema>.pages.site_id` a foreign key referencing `<schema>.sites(id)` on delete cascade). These enforce internal consistency without cross-schema links. Also apply **unique constraints/indexes** as needed (for instance, if site names must be unique per tenant or other business rules).
    * **Ownership & Permissions:** If using per-tenant DB roles (e.g. `role_<tenant>`), grant that role full privileges on the new table or change the table owner to the tenant’s role. Otherwise, ensure the generic `authenticated` role has the proper privileges. No RLS is typically enabled on these tenant tables (each schema is single-tenant by design), but if RLS policies exist (e.g. using the `account_id` column for extra safety), ensure a policy like `account_id = auth.jwt().account_id` is added so only the tenant’s data is visible if cross-schema access is attempted.
    * **Idempotence:** Use `CREATE TABLE IF NOT EXISTS` and conditional `ALTER TABLE` statements so the migration/CLI can be safely re-run. For example, only add a column if it’s missing, and only create a table if it doesn’t exist. This avoids errors if some schemas are already up-to-date.
  * *Performance Considerations:* Avoid expensive catalog scans. For example, fetch the list of schemas once and cache it, rather than querying `pg_catalog` inside a loop for each schema. Batch DDL operations if possible. Given \~1,000 schemas, loop carefully to prevent long locks. Running this in a single transaction is ideal to keep changes atomic (Supabase migrations run in a transaction by default), but be mindful of transaction size.
* **Integration into CI/CD:** Add this migration to the version-controlled migration files (e.g. `YYYYMMDDHHMM_add_core_tables.sql`). If implemented as a CLI command, incorporate it into the deploy pipeline after standard migrations. For instance, after applying SQL migrations, run `ozza-cli refresh-core-tables` to sync all schemas. Document this step so it’s not missed during deployment.
* **Maintenance Mode:** Consider toggling the platform into read-only **maintenance mode** while running this update. Use `public.settings(maintenance_all)` to block writes globally before the migration, then disable it after completion. This ensures no tenant is writing to a table that’s being altered or created, and follows the blueprint’s deployment safety practices.

## Phase 2: Implement Database Triggers for Plan Limits Enforcement

**Goal:** Add triggers that enforce plan limits at the data layer, per blueprint directives. We will create one trigger on the **public.account\_members** table to enforce `plans.max_users`, and one trigger on each tenant’s **sites** table to enforce `plans.max_sites`. All trigger names and functions use clear, lowercase names indicating their purpose.

* **Trigger on `public.account_members` (Max Users per Account):**

  * *Function Definition:* Write a PL/pgSQL function (in the public schema) called something like `fn_enforce_max_users()` that will execute on insert (or relevant updates) on `account_members`. This function will:

    1. **Count Current Users:** Select the current number of members for the account in the new row. For example: `SELECT COUNT(*) FROM public.account_members WHERE account_id = NEW.account_id;`. (If this is a BEFORE INSERT trigger, this count excludes the new member; if AFTER INSERT, it includes them – choose logic accordingly. A BEFORE trigger can simply do `... + 1` to count the new user.)
    2. **Get Plan Limit:** Query the account’s plan limits. Join `public.accounts` to `public.plans` to retrieve `plans.max_users` for this account’s `plan_id`. This is a small, fast lookup (plan data is static). Ensure the account’s record is fetched with proper locking if needed (for consistency, `SELECT ... FOR SHARE` on the account row could be used, though likely overkill for a simple check).
    3. **Enforce Limit:** If `current_member_count + 1 > plans.max_users`, then block the operation. Raise a clear exception, e.g. `RAISE EXCEPTION 'Max users reached - upgrade your plan.';`. This error message will propagate to the client. If the blueprint defines a special allowance during upgrades (e.g. if `accounts.plan_status = 'upgrading'` temporarily allows oversubscription), include that logic: only enforce when `plan_status` is not an upgrading state.
    4. **Optional Update Handling:** If role updates can affect active user count (unlikely in this case, since the count of rows doesn’t change on role change), you might include an `AFTER UPDATE` trigger as well to catch scenarios where a user is being reactivated or moved between accounts. By blueprint, new memberships are inserts only, so focus on INSERT events.
  * *Trigger Creation:* Create a **BEFORE INSERT** trigger on `public.account_members` named e.g. `trg_account_members_max_users` that calls the above function. This ensures the check runs prior to the insert commit. The trigger should fire “FOR EACH ROW” on insert (and on update if implemented).
  * *Security & Permissions:* Since this trigger will execute in the context of the inserting user (often an “Owner” inviting a user), ensure the function can read the necessary data:

    * **Option 1 (Preferred):** The `authenticated` role (or tenant-specific role) should have SELECT access on `public.accounts` (restricted to their own account via RLS) and on `public.plans` (plans are non-sensitive, so allow read to all). In Supabase, this means adding RLS policies or grants such that an authenticated user can select their account row and the plans table. Blueprint guidance notes that plan info is generally readable to all auth users, so we can enable a simple `SELECT` policy on `public.plans` for all roles. The account row is already accessible via RLS (each user can see their own account). This option keeps the trigger function running with invoker rights under RLS.
    * **Option 2:** Mark the trigger function as `SECURITY DEFINER`. Owned by a privileged role (e.g. the database service role), it will bypass RLS and permission checks when executing. This ensures it can always read `accounts/plans` regardless of the inserting user’s privileges. If using this, tightly control the function: it should be declared with `SECURITY DEFINER` and owned by an admin role, and you should `REVOKE EXECUTE ON FUNCTION fn_enforce_max_users FROM public;` so that normal users cannot directly call it. The trigger will still execute it automatically.
    * In either case, test that the trigger’s queries (counting members, selecting plan limits) execute successfully as an authenticated user – adjust grants or use definer as needed so the trigger does not hit a “permission denied” error.
  * *Naming Convention:* Ensure the function and trigger name follow conventions (all lower-case, descriptive). For example, `fn_enforce_max_users` for the function and `trg_account_members_enforce_max_users` for the trigger. This clarity will help future developers quickly identify their purpose.

* **Trigger on `<tenant>.sites` (Max Sites per Account):**

  * *Function Definition:* Create a PL/pgSQL function in each tenant schema (or a generic function that can operate per schema) named e.g. `fn_enforce_max_sites()`. This function will run on the tenant’s `sites` table inserts. Its logic:

    1. **Count Current Sites:** `SELECT COUNT(*) FROM <tenant>.sites;` to count how many sites already exist in that schema (for that account). Since each schema belongs to one account, this implicitly scopes the count to that account’s sites. If an `account_id` column is present in the `sites` table, you could alternatively count `WHERE account_id = <tenant_uuid>` for clarity – all rows should match that anyway.
    2. **Get Plan Limit:** Determine the max allowed sites for this account. There are two ways:

       * Use the account’s `plan_id` to fetch `plans.max_sites` similar to above. You can join `public.accounts` filtering by `accounts.schema_name = current_schema()` to get the account row, then join to `public.plans`.
       * Or, if the `sites` table has an `account_id` field (populated with the account’s UUID), use `SELECT max_sites FROM public.plans p JOIN public.accounts a ON p.plan_id = a.plan_id WHERE a.id = NEW.account_id;`.
         In either case, retrieve the numeric limit `max_sites` for the current plan.
    3. **Enforce Limit:** If `current_site_count + 1 > max_sites` (i.e. the account already has the maximum allowed sites), block the insert. `RAISE EXCEPTION 'Max sites reached - upgrade your plan.';` to abort the transaction. Again, consider if any grace period during upgrades is needed (likely not for sites – use plan\_status if applicable similarly).
  * *Trigger Creation:* Attach this function as a BEFORE INSERT trigger on each tenant’s `sites` table (e.g. `trg_sites_max_sites`). This must be done for **every tenant schema**. You can automate this in the same loop/script from Phase 1: after ensuring the `sites` table exists, create the trigger function and apply the trigger for that schema. For new accounts going forward, update the tenant provisioning logic to include creating this trigger as part of setting up the schema. (If the function name is the same in every schema, it’s scoped by schema and won’t conflict globally. Alternatively, a single `public` function could accept the schema or account as an argument, but per-schema triggers are simplest and align with our one-schema-per-tenant design.)
  * *Security & RLS:* Similar considerations as the user limit trigger: the function will need to read `public.accounts`/`public.plans` to get the limit. In Supabase, an insert into `<tenant>.sites` is done by a user with rights only to that tenant’s schema (and possibly limited public access), so either:

    * Grant that role SELECT on `public.plans` (all plans or at least the relevant plan row) and ensure an RLS policy allows it (or leave `plans` open to read). Also allow the role to select its own account from `public.accounts` (RLS on accounts already allows a user to see their own account). Then run the trigger with invoker rights.
    * Or use a SECURITY DEFINER function owned by a superuser to perform the cross-schema select. Given this trigger lives in the tenant schema, you might prefer the first option (grant minimal read rights) to avoid having to maintain many definer functions. The plan data is not sensitive, and each user already knows their plan limits from the UI, so a read grant here is acceptable.
  * *Naming:* Name the trigger and function clearly per schema, e.g. function `fn_enforce_max_sites()` and trigger `trg_sites_enforce_max_sites`. Since these reside in the tenant schema, prefixing with schema name is not necessary (they’re local to the schema).

* **Global Plan Limit Enforcement:** These triggers implement **server-side enforcement** of plan limits as specified by the blueprint. They ensure that even if a client bypasses UI checks, the database will reject writes that violate the plan constraints, preserving data integrity. This fulfills the founder’s directive that *“critical business rules are enforced at the database level”*.

## Phase 3: Testing & Verification

* **Migration Verification (Core Tables):** After running the Phase 1 migration/CLI, verify that every tenant schema now contains the full set of core tables. Write a script to loop through schemas and assert the existence of `sites`, `pages`, etc., and the presence of key columns (e.g. each `pages` table has a `site_id` column, etc.). Ensure that any newly added `account_id` columns correctly store the tenant’s UUID for existing records. No schema should be left behind.
* **Trigger Behavior Tests (Plan Limits):** Develop tests (automated if possible) to cover scenarios for the new triggers:

  * *Max Users Trigger:*

    * Create an account (or use a staging account) on a plan with a small `max_users` (e.g. Free plan with 1 or 2 users). Attempt to insert one more `account_member` than allowed. The insert should fail with the expected error message (“Max users reached…”). For example, if `max_users = 1` and one member exists, adding a second should be blocked. Verify that under the limit, inserts succeed normally.
    * Test edge conditions: if `max_users` is exactly N, adding the Nth user (when N−1 exist) should succeed, but adding the (N+1)th should fail. If the account’s plan is changed to a higher tier, verify that additional members can now be added (up to the new limit). Also test downgrades: if an account is downgraded to a lower plan with a now-exceeded user count, ensure no new inserts are allowed (existing users may remain – the system might require an account adjustment in such cases, but trigger mainly prevents *new* additions).
    * If an “upgrading” status is used, simulate that state: e.g., mark `plan_status = 'upgrading'` for an account and attempt an insert just over the old limit – it should possibly bypass the check. Then ensure that once `plan_status` is Active on the new plan, the higher limits apply.
  * *Max Sites Trigger:*

    * Similar approach: use an account on a plan with a known `max_sites`. In that tenant’s schema, attempt to insert sites. When at limit, the trigger should throw an exception. If you have a UI for site creation, ensure it surfaces the DB error gracefully.
    * Test creating sites right at the threshold vs. one above. Also test after plan upgrades (e.g., upgrade from Free with 1 site max to Pro with 5 sites max – after upgrade, inserting a second site should now succeed). If the plan is downgraded below the current site count, ensure the trigger disallows any new site until they are back under limit (the existing sites likely remain – perhaps the user should be warned to remove some, but the trigger will stop any *additional* ones).
  * *Cross-Tenant Safety:* Attempt a malicious scenario (with an advanced tester mindset): using an authenticated user’s credentials, try to insert a site into another tenant’s schema or an account member into another account (e.g. by changing the schema or account\_id in a direct API call). The expected result is that either the permission is denied by PostgreSQL (no access to that schema) or the RLS policies and triggers combined prevent it. For instance, if a user somehow tries to set `NEW.account_id` to a different account in account\_members, the RLS policy on insert or a check constraint should reject it. These tests ensure the isolation remains intact alongside the new triggers.
  * *Performance:* Monitor the execution time of these triggers under load. For example, bulk-inserting 50 account\_members should not significantly degrade due to the count query (which is cheap for small N). Ensure no deadlocks: the trigger’s SELECT on account or plans should be quick and not hold locks that contend with other transactions. Write tests that simulate concurrent inserts (if applicable) to ensure the trigger behaves correctly (two concurrent inserts hitting the limit might both see count N and allow, but one should fail when committing – this is a rare race condition to consider).
* **Review Logs & Errors:** After deployment, keep an eye on database logs for any trigger-related errors (e.g. permission denied in trigger function, or unexpected exceptions). If any are found in testing, adjust privileges or logic accordingly and re-run tests.
* **Supabase Policy Checks:** Confirm that the RLS policies did not inadvertently block the triggers’ internal queries. For example, if using invoker rights, ensure the RLS policy on `public.accounts` indeed allows the inserting user to SELECT their own account (it should, by design). Also verify the `public.plans` policy (if any) allows reading limits (for simplicity, you may leave RLS off on plans or have a permissive policy since plan data is not sensitive). Adjust policies as needed and re-test.

## Phase 4: Deployment, Rollback & Ongoing Maintenance

* **CI/CD Integration:** Include the new migration and trigger creation in your migration files and CLI tools. Update the **ozza CLI** if needed: e.g., add `ozza-cli refresh-core-tables` to run Phase 1, and ensure `ozza-cli create-tenant` (or tenant provisioning script) now creates the `sites` trigger in any new schema by default. The CI pipeline should run migrations as part of deploy, and possibly run a post-deploy check to confirm these triggers are active (a canary test could attempt an over-limit insert and expect a failure, for instance). This aligns with blueprint’s safe deployment practices (post-deploy tests and no manual DB changes outside migrations).

* **Rollback Strategy:** In line with the blueprint’s additive-only migration policy, prefer **rolling forward** with fixes over rolling back. If a rollback is absolutely necessary (e.g. a critical bug), since we haven’t dropped or altered existing data, we can disable or remove the new features temporarily without data loss. Steps could include:

  * Dropping or disabling triggers if they misbehave (e.g., `DROP TRIGGER trg_account_members_max_users ON public.account_members;`). This would temporarily lift the limit enforcement until a fix is applied. Similarly, drop the `trg_sites_max_sites` in each schema (a script can automate this). *Note:* Only do this if the bug is severe; document any period during which limits aren’t enforced and reconcile after.
  * If newly created tables are causing issues (unlikely, since they’re just empty structures when added), you can leave them in place (preferred, they do no harm) or drop them in a backward migration. Dropping tables should be done only if absolutely required, given the “never destructive” principle. If dropped, ensure to cascade drop dependent objects like triggers on them.
  * Because no existing data is removed by our changes, reverting the application code to a previous version will simply mean the new tables/triggers lie dormant. This is safe – they’ll just not be used by old code. Thus, rollback can often be achieved by deploying old code while leaving the DB as-is, then quickly fixing forward.

* **Supabase RLS/Policy Maintenance:** After deployment, audit the RLS policies and role grants one more time in production. Ensure that the `authenticated` role (or relevant roles) still cannot perform any cross-tenant data access except through allowed paths. The new triggers should be invisible to users security-wise (they either succeed or throw a generic limit error). Make sure no policy needs to be added for the new core tables: typically, tenant tables in separate schemas may not use RLS, but if you decide to add RLS on them using the `account_id` column for belt-and-suspenders isolation, add those policies during this migration as well (e.g., on each `<schema>.sites`, `CREATE POLICY tenant_isolation ON <schema>.sites USING (account_id = auth.jwt().account_id);`). According to the blueprint, isolation is primarily achieved at the schema level or via roles, so per-table RLS in each schema is optional.

* **Documentation & Blueprint Conformance:** Update internal documentation to reflect these changes. The blueprint (v5) already describes these triggers and tables, so ensure our implementation comments in code reference the blueprint sections (for future maintainers). The solution strictly follows the architectural guidelines: multi-tenant additive schema updates and server-side plan enforcement. No creative deviations were introduced. All naming (table names, trigger names, CLI commands) is consistent with our established conventions.

* **Future Monitoring:** Once in production, monitor if any account hits their limits frequently (the trigger exceptions can be logged). This could inform upsell opportunities or the need to adjust plans. Also, watch for any performance issues (none expected, as counts are fast). As the tenant count grows, periodically test that the refresh-core-tables script and triggers creation scale (the blueprint notes that thousands of schemas are supported, but heavy cross-schema operations should be optimized). We have built the migration/CLI to be efficient per those guidelines.

By executing the above checklist, we ensure all existing tenants have the required schema structure and that **plan limits are enforced at the database level** in accordance with the Ozza Platform Blueprint v5. This implementation is comprehensive, avoids ambiguity, and upholds the platform’s multi-tenant integrity and security requirements.
