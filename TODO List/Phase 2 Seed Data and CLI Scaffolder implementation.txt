Understood. I’ll now create a comprehensive and unambiguous TO DO list for the Seed Data and CLI Scaffolder implementation—including the commands `db:migrate`, `db:reset`, `seed`, and `create-tenant`. This will follow the Website Blueprint V5 precisely, detailing all the exact operations needed with no assumptions or creativity required by the implementer. I’ll let you know as soon as it’s ready.


# Ozza CLI Seed Data & Scaffolder Implementation – Complete To-Do List

## CLI Command: `ozza-cli db:migrate`

* **Set Up Migration Framework:**

  * Install and configure the Supabase CLI (ensure it’s authenticated and pointed to the correct project/database URL). Set up a `/supabase/migrations` directory for SQL migration files, using Supabase’s timestamped naming convention (e.g. `YYYYMMDD-HHMM-description.sql`). Verify the Supabase config (like `supabase/config.toml` or env connection string) is in place so the CLI can run migrations against the database.
  * Implement a **pre-push Git hook** or CI check that scans new migration files for destructive commands (`DROP`, irreversible `ALTER`, etc.) and blocks them. This enforces the founder’s directive that schema changes must be additive-only.
* **Create Core Public Tables (Additive Migration):**

  * Write an initial SQL migration that **creates all required `public` schema tables** according to the blueprint’s data model. This includes:

    * **`public.accounts`:** Table with one row per tenant account. Fields should include at least `id UUID PRIMARY KEY`, `account_name` (agency name), `schema_name` (unique identifier for the tenant’s schema), `plan_id` (FK to `plans`), `plan_status` (e.g. active, past\_due), `stripe_customer_id`, `maintenance_mode` (boolean), timestamp columns, etc. Enforce a UNIQUE constraint on `schema_name` (since each tenant schema must be unique). Add a foreign key on `plan_id` referencing `public.plans.plan_id`. Enable Row-Level Security and add policies so that logged-in users can `SELECT` *only their own* account row (using the JWT’s `account_id` claim to match `accounts.id`). No user should see or modify another tenant’s account record.
    * **`public.account_members`:** Table linking Supabase Auth users to accounts (many-to-many). Fields: `account_id UUID` (FK to accounts), `user_id UUID` (from `auth.users`), `role` (text, e.g. Owner, Admin, Coach, Member), plus metadata like `created_at`. Use a composite primary key (`account_id, user_id`) or an ID PK. Enable RLS policies so that users can see rows only for their own account. Specifically, a user can view members of *their* account (to list their team) but not any other account’s members. (The RLS policy can allow `SELECT` on rows where `account_id = auth.jwt() ->> 'account_id'` or ensure membership via a join to account\_members itself.)
    * **`public.plans`:** Table defining subscription plans (e.g. Free, Pro). Fields: `plan_id` (text or UUID PK, e.g. `"free"`, `"pro"`), `name` (display name), pricing info (`monthly_price`, `annual_price`, `stripe_price_id`, etc.), and **limit columns** for key resources (e.g. `max_users`, `max_sites`, etc.). Populate this table with the known plans and their limits as part of initial setup. For example, seed a “Free” plan (e.g. max\_sites = 1, max\_users = 1, etc.) and a “Pro” plan (higher limits, additional features). (This static data seeding can be done via a migration SQL insert or in the separate seed script – see the `seed` command steps – but ensure that at least these records exist after initial migration.) Grant read access on `plans` to all authenticated users (either via a liberal RLS policy or simply by not enabling RLS on this table) so the front-end can fetch plan details for display.
    * **`public.features`:** Table listing all feature flags/gates in the system. Fields: `feature_key` (text PK, e.g. `CUSTOM_DOMAIN`, `WHITE_LABEL`, `AI_CONTENT_ASSISTANT`), `description` (text), and optionally a default or plan-association indicator. Insert initial feature definitions here (e.g. features for all togglable capabilities, like custom domains, white-label branding, analytics dashboard, AI assistant, etc.). This table is mostly reference data for internal logic, so it can be readable by all authenticated users or restricted to service role – it contains no sensitive tenant info. (We will use this table to know which features to enable per plan/account.)
    * **`public.account_features`:** Table mapping accounts to feature flags and their enabled status. Fields: `account_id` (FK to accounts), `feature_key` (FK to features), `enabled` (boolean). Together with `features`, this table lets us turn features on/off per account (for plan-based entitlements or experimental features). Implement a composite PK on `(account_id, feature_key)`. Enable RLS so that accounts can see only their own feature flags. Initially, this table will be filled when seeding or creating tenants (see the `seed` and `create-tenant` tasks) – we don’t insert rows here in the migration except perhaps for a default template account if one is used.
    * **`public.domains`:** Table for custom domain mappings. Fields: `domain` (text PK), `account_id` (FK to accounts), possibly `site_id` if a domain is tied to a specific site. This table is used at runtime to route requests by domain to the correct tenant. Enable RLS so that tenants can only see their own domain entries. (Likely, a policy allows selecting domains where `account_id = auth.jwt() ->> 'account_id'`.)
    * **`public.settings`:** Table for global platform settings (one-off flags and config). Fields: `key` (text PK), `value` (text or JSON). Use this for system-wide toggles like `maintenance_all` (to put the whole platform read-only) or `allow_signups` (to globally enable/disable new user registrations). Seed this table with essential defaults: e.g. insert `('maintenance_all', 'false')` and `('allow_signups', 'true')` so the system knows the default state. RLS can be tight here (perhaps only platform admins or service role can select/update), since regular users don’t need direct access to global settings.
    * **`public.billing_events`:** Table to log processed Stripe webhook events (for idempotency and audit). Fields: `event_id` (text PK from Stripe), `event_type`, `account_id` (if applicable), `processed_at` timestamp. This is maintained by the billing webhook function. No RLS needed if it’s only accessed by service role, or restrict by account if exposing to tenants (not likely). Create a unique index on `event_id` to prevent duplicates.
    * **(Optional) `public.audit_log`:** If specified in the blueprint for cross-tenant access auditing, create an `audit_log` table (fields like `id, performed_by_user_id, target_account_id, action, timestamp, details_hash`) to record whenever a privileged user (coach or admin) accesses another tenant’s data. Ensure this is ready for future use in “coach proxy” or admin support actions. Regular users won’t access this, so no general RLS (or restrict to admins).
  * Define **constraints and indexes** in each table as needed. For example, ensure unique indexes where appropriate (e.g. unique `account_name` or subdomain if required to prevent duplicates, unique `(account_id, feature_key)` for account\_features, etc.). Add foreign key constraints for referential integrity on things like `accounts.plan_id -> plans.plan_id` and `account_features.feature_key -> features.feature_key`. (We avoid cross-schema FKs, but within `public` schema it’s fine.)
* **Implement Role & RLS Configuration:**

  * Enable Row-Level Security on all tables that require tenant isolation (all the above `public` tables except perhaps `plans` and `features` which are global reference data). Write RLS policies in the migration SQL to enforce the multi-tenancy rules:

    * For `accounts`: allow each authenticated user to `SELECT` their own account row (policy using `id = auth.jwt() ->> 'account_id'`). Typically only admins or service roles can insert/update accounts (sign-ups and plan changes are via privileged paths).
    * For `account_members`: allow users to `SELECT` members where `account_id = auth.jwt() ->> 'account_id'` (so they can list their team). Possibly allow inserts only via a secure function or service role (since invites are controlled). Owners/Admins might have an update/delete policy to manage members in their account as needed.
    * For `account_features`: allow `SELECT` for rows where `account_id = auth.jwt() ->> 'account_id'` (so an account can read its feature flags). Inserts/updates here will be done by internal logic (e.g., during account creation or plan upgrade via service role), so direct tenant writes may not be allowed except via controlled endpoints.
    * For `domains`: allow `SELECT` for rows where `account_id = auth.jwt() ->> 'account_id'` (each tenant sees only their domains). Inserts/updates will be via our API when a tenant sets up a custom domain, likely requiring Owner role – implement those checks in application logic or add a restrictive RLS policy for insert (e.g., allow if `account_id = my account AND auth.jwt() ->> 'role' = 'owner'`).
    * For `settings`: no general tenant access (this is global). Possibly allow read access to certain settings for authenticated users if needed (e.g., the app might read `allow_signups` or other public settings without special privilege). Otherwise, restrict this table to service role or platform admin.
    * For tenant-specific tables created later in each schema (sites, pages, etc.), plan to enable RLS in those schemas as well (see `create-tenant` steps). We will leverage the `account_id` field on those tables and JWT claims to isolate tenant rows in their own schema.
  * **Database Roles:** For MVP, all end-users share the Supabase `authenticated` role, and we rely on RLS policies using the `account_id` claim to prevent cross-tenant access. (No need to create per-tenant Postgres roles at this stage, as the blueprint notes we can stick to a generic role with RLS for simplicity.) Ensure the migration creates any *platform roles* needed: e.g., a `platform_admin` Postgres role if we want to grant it broader read access, though in practice we’ll likely just use the service role JWT for admin operations. Grant the `authenticated` role requisite privileges on the new tables (e.g., `GRANT SELECT, INSERT, UPDATE, DELETE ON public.accounts TO authenticated`, etc.) so that RLS policies can take effect. For tables where we want open read (plans, features), also grant `SELECT` to `authenticated` (or set a policy that TRUE for read).
* **Apply Migrations via CLI:** Implement the `ozza-cli db:migrate` command to run the above SQL migrations against the database. For example, invoke `supabase db push` or `supabase db migrate up` programmatically. The CLI should output the progress of applying each migration. Include error handling – if a migration fails, log the error and exit. After running migrations, consider having the CLI refresh or regenerate the Supabase `schema.sql` (if using Supabase’s dev workflow) to reflect the latest structure.
* **Post-Migration Schema Verification:** After `db:migrate`, verify that the schema is set up correctly:

  * Confirm all `public` tables were created and have the correct columns and constraints (e.g. check that `accounts`, `plans`, etc. exist). You can use the Supabase CLI (`supabase db meta`) or a direct query (e.g., `SELECT * FROM information_schema.tables`) to list tables.
  * Verify RLS is enabled on tables that require it (e.g., query `pg_policy` or attempt a cross-tenant select as a test user to ensure it’s blocked). For instance, try selecting another account’s data using a user’s JWT – it should return no rows or an error.
  * If any *initial data* inserts were included in the migrations (like plans/features seeds), ensure those records exist (e.g., query `plans` table to see “Free” and “Pro” plans). This ensures the next step (seeding) won’t accidentally insert duplicates.
  * Run any included migration tests or smoke tests: e.g., if a post-deploy canary schema (`deploy_canary_<commit>`) is created to simulate critical flows, ensure it ran successfully. (For now, in development, you might manually create a test tenant after migration to ensure the process works end-to-end.)
* **Documentation:** Update the project README or docs to note that `ozza-cli db:migrate` runs all pending migrations. Emphasize the additive-only migration rule and how the CLI enforces it (so all developers know to use this command and not raw SQL outside it).

## CLI Command: `ozza-cli db:reset`

* **Safety Confirmation:** Implement a **double confirmation** for this dangerous action. When `ozza-cli db:reset` is invoked, the CLI should warn that this will **destroy all tenant data** and prompt the user to confirm (e.g., requiring the user to type “RESET” or pass a `--force` flag) before proceeding. Additionally, detect if the environment is production (via an env variable or Supabase project ID) and, if so, refuse or extra-confirm the reset to avoid accidental production data loss.
* **Drop All Non-System Schemas:** Connect to the database with a privileged role (using the Supabase service role connection string or the `supabase` CLI). Enumerate all schemas that are not system schemas and not the shared `public` schema. In a Supabase Postgres instance, system schemas include `information_schema`, `pg_catalog`, `pg_toast`, as well as Supabase-specific schemas like `auth`, `storage`, `extensions`, etc. Only schemas corresponding to tenant accounts should be dropped. One reliable method: query the `public.accounts` table for all values in `schema_name`, and drop each of those schemas (since each account’s schema\_name corresponds to a tenant schema). Execute `DROP SCHEMA <tenant_schema> CASCADE` for each, logging progress. (Do **not** drop the `public` schema or any Supabase-managed schemas – we keep `public` and system schemas intact.) This effectively deletes all tenant-specific tables and data, but preserves our shared reference data and auth tables.

  * *Note:* If the `public` schema itself contains data that we want to reset (plans, features, etc.), we will handle re-inserting/updating those in the seeding step. We generally do not drop `public` itself to avoid messing up the core schema; instead, we rely on idempotent seed inserts to refresh reference data. If needed (for a truly clean slate), one could truncate or drop rows from certain public tables (like `accounts`, `account_members`) at this point, but since we’ll be re-running migrations, it’s simpler to drop and recreate those tables via migration. In practice, because we run migrations from scratch next, the `public` tables will be re-created empty, so their old data can be discarded by dropping the tables or the whole schema.
* **Recreate Core Schema via Migrations:** After dropping tenant schemas, run the full set of database migrations from scratch. The `db:reset` command should internally invoke the same steps as `db:migrate`, but starting on a fresh state. One approach is to **reinitialize the database**: for example, using Supabase CLI’s reset if available (e.g., `supabase db reset` which drops and re-creates the local dev database). In a remote environment, or to be precise, explicitly run down and up migrations:

  * If possible, use a fresh database or drop all user-defined tables in `public` as well (to avoid conflicts when reapplying migrations). Ensure that the `public` schema is clean – you may drop the `public` tables created by previous migrations (or even drop and recreate the `public` schema) before running migrations again.
  * Then call the migration runner (like `supabase db push` or apply each SQL file in order) to rebuild the schema from zero. This will recreate all `public` tables, roles, and policies as defined in migrations. Check that this completes without errors. All baseline data structures (accounts table, etc.) should now be exactly as in a brand new installation.
* **Re-run Seed Scripts:** After migrations, the `db:reset` command should automatically execute the seeding routine (equivalent to running `ozza-cli seed`). This will repopulate reference data and initial accounts (founder admin, etc.) as described in the next section. Make sure that the seeding logic is idempotent and can run on a fresh schema (it should detect that no data exists and insert defaults). The CLI should output that it is now seeding the database after migration.
* **Post-Reset Verification:** Verify that after `db:reset`, the database is in the expected initial state:

  * Query the `public.accounts` table – it should contain only the baseline accounts that the seed script adds (e.g., the platform admin account, and possibly a demo account if in a dev environment). All previously created tenant rows should be gone. Likewise, the only schemas present should be `public` (and system schemas, plus possibly a demo schema if seeded).
  * Check that the reference tables are correctly populated: the `plans` table should have the Free and Pro plans, `features` table should list all features, etc. (The seed step should have inserted these; verify their presence).
  * Ensure the founder admin user exists in the auth system and is linked in `account_members` to the admin account. Also verify any demo user/account if those were supposed to be created in this environment.
  * If applicable, run a quick end-to-end sanity test: for example, use `ozza-cli create-tenant` to add a new test tenant after the reset and confirm it works (this ensures that the migrated schema + seed data produce a working system). All automated tests or smoke tests should also be run to confirm no regressions.
* **Documentation & Environment:** Document that `ozza-cli db:reset` is primarily for development/test environments. Emphasize the **SEED\_DEMO\_DATA** flag (or environment difference) that controls whether demo content is created – in production, this command would typically not be run (or would run without demo data). Include guidance that in development, running `db:reset` is a convenient way to get a clean database with all seeds (e.g., “Run `ozza-cli db:reset` to initialize your local dev database with schema and sample data”).

## CLI Command: `ozza-cli seed`

* **Purpose & Idempotency:** Implement the `seed` command to populate static reference data and initial records into an existing database. This command can be run on a fresh database (e.g., right after migrations) or on an existing one to backfill new reference entries. Design the seeding queries to be **idempotent** – i.e., running them multiple times should not cause duplicate key errors or conflicting data. Use `INSERT ... ON CONFLICT DO NOTHING` or checks in code before insertion to skip already-seeded items. This allows `ozza-cli seed` to be run safely at any time (especially useful after a `db:reset` or when introducing new features/plans in a future release).
* **Load Environment & Connect:** Ensure the CLI loads necessary environment variables, such as a Supabase Service Role key and API URL, because seeding will involve privileged operations (like creating a user via the Supabase Auth API). Also read the flag `SEED_DEMO_DATA` from env to decide on demo content seeding. Typically, in development `.env`, you’d set `SEED_DEMO_DATA=true`, and in production `.env`, `SEED_DEMO_DATA=false` to avoid demo/test data in prod.
* **Seed Plans Table:** Insert the predefined subscription plans into `public.plans` (if not already present). For MVP, seed at least:

  * **Free Plan:** e.g., `plan_id = 'free'`, name "Free", with limits such as `max_sites = 1`, `max_users = 1`, etc., and price \$0. Include any additional default attributes (perhaps `stripe_price_id` for Stripe integration, though in test it can be a placeholder).
  * **Pro Plan:** e.g., `plan_id = 'pro'`, name "Pro", with higher limits (e.g., `max_sites = 10`, `max_users = 5` – adjust based on blueprint specs) and a Stripe price ID for the paid plan. This plan likely enables advanced features (custom domain, white-label, etc.). Ensure the inserted values match exactly the blueprint’s defined plan structure.
  * Use `ON CONFLICT DO NOTHING` so if these plans already exist (from a previous seed run or migration), the inserts will be skipped without error.
* **Seed Features Table:** Insert all feature flag definitions into `public.features`. For each planned feature/toggle in the platform, add a row with a unique `feature_key` and description:

  * Examples from the blueprint: `CUSTOM_DOMAIN` (ability to use custom client domains), `WHITE_LABEL` (remove Ozza branding), `ANALYTICS_DASHBOARD`, `AI_CONTENT_ASSISTANT`, etc.. Provide a clear description for each so its purpose is documented in the DB.
  * If the blueprint specifies a default availability for the feature (e.g., which plans include it), and we have a column or convention for that, fill it in. Alternatively, maintain a mapping in the seeding logic (see below when assigning features to accounts).
  * Again, make these inserts upsert or idempotent. Only add features that aren’t already present.
* **Seed Initial Account(s) and User(s):** The seed process will create critical initial principals:

  * **Platform Admin (Founder) Account & User:** As per founder directive, create a special “platform admin” user account that has oversight of the system.

    * Use the Supabase **Admin API** or JS client with service key to create a new auth user for the founder. The email and default password for this account can be taken from environment variables (e.g., `FOUNDER_EMAIL`, `FOUNDER_PASSWORD`) to avoid hardcoding secrets. If not provided, document that manual step may be required to set the password. Call the Supabase Auth admin function to create this user (e.g., `auth.admin.createUser({ email, password, email_confirm: true })` using the Supabase JS library).
    * Once the user exists in `auth.users`, insert a new row in `public.accounts` for the platform admin’s account (if not already present). For example, `account_name = 'Platform Admin'` (or the founder’s name), with a new UUID `id` and perhaps `schema_name = NULL` or some identifiable string like `'admin'`. Since this account is not a normal tenant, you may choose not to create a dedicated schema for it (the founder doesn’t run a site/funnel agency). The blueprint leaves this to design: one approach is to give the admin a schema for formality but it won’t be used; another is to leave `schema_name` empty or a dummy value and treat this account specially in code. (If implementing strictly, you might create an `admin` schema for completeness, but ensure it doesn’t interfere with normal tenants.)
    * Set this admin account’s `plan_id` to perhaps `'free'` or a special plan (if the founder shouldn’t have limits). The blueprint suggests the founder account is not used like a normal tenant, so the plan might be inconsequential – using Free is fine for now. `plan_status` can be `'active'`. Other fields: no Stripe customer needed (they’re not a paying client), and `maintenance_mode` default false.
    * Insert an `account_members` row linking the founder’s new `user_id` to the admin account’s `account_id` with a role. Use `role = 'Owner'` or `'PlatformAdmin'` as appropriate. (If we use a standard role like Owner, we’ll know this specific account is the platform admin by its context; or define a distinct role for clarity.) This membership gives the founder’s user a JWT `account_id` (the admin account) and role on login.
    * **Ensure idempotency:** if the founder user or account already exists (e.g., we run seed twice), detect that and skip or update as needed. For example, search `auth.users` by email to see if the user is already there, and query `public.accounts` for an account with that name or a known ID. Do not create duplicates – instead, log that founder admin already exists.
  * **Reference Data Consistency:** After creating the founder account, you may seed any **account-specific defaults** needed for it. Since the founder isn’t a typical tenant, we might not assign feature flags or plan limits, but if the system expects entries in `account_features`, we could insert them. (For example, if our code checks `account_features` table even for the admin account, we might give the founder account all features enabled by default to avoid any restriction, or simply treat it separately in logic.)
* **Optional Demo Tenant (Dev/Staging only):** If `SEED_DEMO_DATA=true`, the seed script should create a fully working **demo tenant** with sample data for testing and sales demos. This typically includes:

  * **Demo User:** Create a new Supabase Auth user to own the demo account (e.g., `demo@yourapp.com` with a known password). Use Auth admin API as with founder. Mark email confirmed. (Alternatively, reuse the founder user as owner, but it’s cleaner to have a distinct user for demo).
  * **Demo Account:** Insert a new row in `public.accounts` for the demo agency. Set `account_name` to something like "Demo Agency" (or a more descriptive name if provided in blueprint). Generate a unique `schema_name` for it (e.g., `"demo"` or a UUID) and ensure it doesn’t conflict with others. Assign this account the `'pro'` plan so it has full features for demonstration. `plan_status = 'active'`, and you can leave `stripe_customer_id` empty (since this is a dummy account not truly billed).
  * **Tenant Schema & Tables:** Create a new schema in the database for the demo tenant (e.g., `CREATE SCHEMA demo;`). Then, either reuse the logic of `ozza-cli create-tenant` or explicitly create the required tables in that schema (sites, pages, leads, etc.) as described in the next section. Populate this demo schema with **mock data**: for instance, create a sample site (`demo.sites` row with a name like "Example Site"), a sample page (`demo.pages` linked to that site, with some dummy content or title), perhaps a form and a couple of `demo.leads` (fake leads captured) to simulate activity. This gives the demo account something to showcase.
  * **Account Membership:** Link the demo user to the demo account in `public.account_members` with role `'Owner'`. This way, the demo user can log in and see the demo content. (If desired, also link the founder user to this demo account with a role like `'Coach'` or `'Admin'` so the founder can access it via the coach portal or admin tools, but this is optional and would be done carefully if at all).
  * **Feature Flags for Demo:** Since the demo account is on Pro plan, seed its `account_features` entries accordingly (likely all features enabled). Insert rows into `public.account_features` for each `feature_key` with `account_id = demo_account_id` and `enabled = true/false` based on Pro defaults. E.g., for `WHITE_LABEL`, `CUSTOM_DOMAIN`, etc., set `enabled = true` (Pro includes them). For any experimental features that are generally off, you can leave them false to mimic a typical Pro account.
  * Wrap all demo seeding in checks so it doesn’t run if `SEED_DEMO_DATA` is false (meaning a production or secure environment).
* **Assign Feature Flags to Existing Accounts:** Whether for the newly created admin/demo accounts or for all accounts in general, ensure that every account has the appropriate entries in `account_features` representing their plan’s features. On initial seeding, that means:

  * For the **platform admin account** (if applicable): possibly skip, or give all features = true if we treat it as unrestricted. (This account may be handled specially in code, so this step is optional.)
  * For the **demo Pro account**: insert rows for every feature with `enabled` set according to Pro plan defaults (true for premium features, maybe true for all since Pro gets everything, unless some flags are experimental and remain off).
  * The seeder could also proactively set up a template for Free plan accounts (even if none yet exist, aside from possibly the founder if treated as Free). For example, if we had any Free accounts present, we would insert `account_features` with `enabled=false` for Pro-only features for those accounts. Since on initial setup the only real tenant might be the demo (Pro) and founder (admin), this may be minimal. However, **design the seeding logic to handle future accounts**: i.e., if `ozza-cli seed` is run on a populated database (imagine we added a new feature flag via migration), it should add a corresponding `account_features` row for all existing accounts with the correct default. This implies iterating over all accounts in `public.accounts` and for each new `feature_key` in `public.features`, inserting a row with `enabled = false` or true depending on plan. *Example:* if a new feature `AI_CONTENT_ASSISTANT` is added and by default it’s off for all plans, the seed script would insert `enabled=false` for that feature for every account (unless some accounts are flagged to get it).
  * Use conflict-ignoring inserts or check for existing `account_id`+`feature_key` to avoid duplicating entries. We essentially ensure that after seeding, for every combination of account and feature that should have a record, one exists. (The codebase might treat missing row as feature off, but the blueprint leans toward explicitly storing even “off” states for clarity.)
* **Final Checks in Seed Process:** After inserting plans, features, accounts, and account\_features, the `seed` command should output a success message. Verify:

  * The `plans` table now has the correct number of plans (and no duplicates). The `features` table has all expected feature keys.
  * The founder admin user was created (if not existing) and can be seen in the auth system. The `accounts` table has the admin account row (with expected values) and possibly the demo account. `account_members` should have entries linking the founder user and demo user to their accounts.
  * For each account in `accounts`, there are corresponding rows in `account_features` (e.g., query `SELECT feature_key, enabled FROM public.account_features WHERE account_id = '<some id>';` and ensure it lists the right toggles).
  * If a demo account was created, try logging in as the demo user (manually, using the provided credentials) to ensure the account is functional: the user’s JWT should carry the demo `account_id`, and using the app or a psql query with that JWT, you should be able to read the demo schema’s data (sites/pages) and not any other schema’s data. This validates that RLS and permissions are correctly in place.
  * Run the seed command twice in a row (on a dev database) to confirm idempotency: the second run should find everything already seeded and thus make no new changes (no duplicate plan or feature entries, etc.), and it should exit cleanly without error.
* **Usage Notes:** Document that `ozza-cli seed` can be used to apply new reference data. For example, if a new feature flag is added in a release, a developer or deployment script can run `ozza-cli seed` to insert the new feature into `public.features` and add default entries for all accounts. This prevents the need for a one-off SQL script in such cases. Also note that `db:reset` automatically calls this, so developers typically don’t need to run `seed` manually on a fresh dev setup – it’s more for maintenance or after partial migrations.

## CLI Command: `ozza-cli create-tenant <name>`

* **Input Handling:** The CLI should accept an `<name>` argument for the new tenant’s name (e.g., the agency or company name). Validate that a name is provided. Optionally, allow flags for additional info, such as `--owner <user_email_or_id>` to specify an owner user for the new tenant, or `--plan <plan_id>` to specify a plan (default to “free” if not given). If no owner user is specified, decide whether to create the tenant with no members (and warn that someone must be added later) or to create a new user interactively. (In the typical flow this command might be used after having a user signup, so an owner is usually present.)
* **Generate Unique Schema Name:** Based on the provided account name, generate a valid Postgres schema name for the tenant. This can be a slugified version of the name (lowercase, no spaces or special chars). Ensure uniqueness against existing schemas:

  * Check `public.accounts.schema_name` to see if the slug is already taken. If so, you might append a numeric suffix or use the account’s UUID as the schema name to guarantee uniqueness. The blueprint suggests possibly using the UUID directly as schema\_name for simplicity. For example, if name "Acme Co" yields slug "acmeco" but that's taken, use the new UUID (like `tenant_abcd1234`) or another unique identifier.
  * Make sure the schema name is <=63 chars (Postgres limit) – using a UUID is fine.
* **Create Account Record:** Using a DB service-role connection, insert a new row into `public.accounts` for this tenant:

  * `id`: generate a new UUID for the account (the CLI can generate one or use `gen_random_uuid()` in SQL).
  * `account_name`: the provided name.
  * `schema_name`: the unique schema identifier chosen.
  * `plan_id`: default to `"free"` plan for new signups (unless a different plan was explicitly specified).
  * `plan_status`: set to `'active'` (assuming they start on an active Free trial/plan).
  * Other fields: `stripe_customer_id` can be left NULL initially (we’ll integrate with Stripe on first checkout), `maintenance_mode` default to FALSE, plus set `created_at/updated_at` timestamps.
  * (If the platform uses additional default settings or branding fields in the accounts table, set those to defaults as well, per the blueprint’s account schema.)
  * Ensure this insertion obeys RLS or use the service role (since a normal user wouldn’t have permission to insert into accounts). Using the service key connection will bypass RLS.
  * After insert, retrieve the new `account_id` (the UUID).
* **Create New Schema in Database:** Execute a `CREATE SCHEMA <schema_name>;` for the tenant’s schema. This requires a privileged role. Verify that the schema was created successfully. (In case of error, e.g., name collision, handle it – but we already checked for uniqueness.)
* **Create Baseline Tenant Tables:** Within the new schema, create all the necessary tables for the tenant’s data model. According to the blueprint, each tenant schema includes tables such as:

  * `<schema>.sites` – stores site/portal records for that agency.
  * `<schema>.pages` – pages within sites (with `page_id`, `site_id` FK to sites, title, content, etc.).
  * `<schema>.forms` – (if applicable) form definitions on pages.
  * `<schema>.leads` – leads or form submissions captured (with fields for contact info, etc.).
  * Any other domain tables needed (the blueprint mentions possibly media, analytics, etc., but for MVP, we implement the core ones above).
  * For each table, define columns and constraints per the blueprint’s specification. For example, `sites` might have `id UUID PK`, `account_id UUID`, `name text`, etc.; `pages` might have `id UUID PK`, `site_id UUID REFERENCES <schema>.sites(id)`, `account_id UUID`, `title text`, etc.
  * **Include an `account_id` column in each tenant table** as suggested for clarity and indexing. Set the default of this column to the current account’s UUID (so any insert will auto-fill with this account’s id). This is useful for cross-schema queries or triggers, and for RLS policies.
  * Apply any needed **foreign keys** within the schema (e.g., `pages.site_id -> sites.id`). We avoid cross-schema foreign keys, but within the same schema it’s fine to enforce referential integrity.
  * Create indexes to optimize queries (e.g., index leads by form\_id or pages by site\_id as needed).
  * This step can be achieved by executing a prepared SQL template or file containing all `CREATE TABLE` statements with the schema name substituted. The CLI can maintain a SQL file (or use the `schema.sql` as a template) that lists the standard tenant tables DDL, and perform a text replace for `<schema_name>`.
* **Set Up Tenant-Level RLS (if applicable):** Even though each tenant’s data is in a separate schema, for defense-in-depth we enforce RLS on those tables as well:

  * Enable row-level security on `<schema>.sites`, `.pages`, `.leads`, etc.
  * Create RLS policies similar to the public tables: allow access only to rows matching the tenant’s account id. For instance, on each table, add `USING (account_id = auth.jwt() ->> 'account_id')` for SELECT/UPDATE/DELETE, and for inserts, either rely on the default that will set account\_id or add a CHECK policy that NEW\.account\_id = auth.jwt() ->> 'account\_id'. This ensures that even if a malicious user somehow queries a different schema’s table, they won’t see data (the row account\_ids won’t match their JWT).
  * Note: In Supabase’s model without per-tenant DB roles, a user connected as `authenticated` technically could attempt to query another schema. By not granting usage on other schemas and by these RLS policies, we ensure they cannot read or write cross-tenant data. We also have triggers to double-check account consistency (next step).
* **Data Consistency Triggers:** Implement triggers to enforce that data in tenant schema tables cannot be mis-assigned:

  * Create a function in the new schema (or a generic one in public) that **verifies `NEW.account_id` matches the tenant’s actual account id**. The actual account id for the schema can be obtained since we know it (from the accounts table or from an argument). For simplicity, the CLI can inject the account UUID directly in the trigger definition.
  * For each tenant table (sites, pages, leads, etc.), add a `BEFORE INSERT OR UPDATE` trigger that calls this function. If `NEW.account_id` does not equal the expected UUID for this schema, the trigger raises an exception and aborts the operation. This is an extra guard against any code bug that might attempt to insert data from one tenant into another’s schema.
  * Example: `CREATE OR REPLACE FUNCTION <schema>.enforce_account_uuid() RETURNS trigger LANGUAGE plpgsql AS $$ BEGIN IF NEW.account_id::text <> '<ACCOUNT_UUID>' THEN RAISE EXCEPTION 'Account mismatch'; END IF; RETURN NEW; END; $$;` then `CREATE TRIGGER trg_enforce_account BEFORE INSERT OR UPDATE ON <schema>.pages FOR EACH ROW EXECUTE FUNCTION <schema>.enforce_account_uuid();` (and similarly for other tables). Using the account’s actual UUID hard-coded is acceptable since it’s specific to that schema. This way, even a privileged query that attempts cross-account insert will fail.
  * Additionally, consider triggers for **plan limit enforcement** at the schema level: e.g., on `<schema>.sites` before insert, count existing sites and compare with `public.plans.max_sites` for this account. This might require the trigger to read `plans` via a security definer function or have the tenant schema user be allowed to select its plan info. The blueprint suggests using a SECURITY DEFINER function or granting minimal rights to read plan limits. For MVP, you can implement a simpler version: a trigger on `sites` that selects `plans.max_sites` by joining `public.accounts` (get plan\_id) -> `public.plans`, and if the count exceeds the limit, raise an error. Do likewise for `account_members` (but that table is global, so the trigger for max users per account will be in the public schema, implemented in migrations).
  * Ensure these triggers are created as part of tenant provisioning so the new schema immediately has the same business rule enforcement as others.
* **Grant Permissions:** Grant the appropriate privileges on the new schema’s tables to the `authenticated` role. Since we are (in MVP) using the same Postgres role for all users, we need to allow that role to use this new schema and its tables:

  * `GRANT USAGE ON SCHEMA <schema_name> TO authenticated;`
  * For each table in the schema, `GRANT SELECT, INSERT, UPDATE, DELETE ON <schema_name>.<table> TO authenticated;`.
  * These grants, combined with the RLS policies, mean any authenticated user *could* attempt to access the table, but RLS will restrict actual access to those with matching account\_id. Without the grants, the user wouldn’t be able to use their own schema either. So this step is crucial for the tenant’s own users to function.
  * (If in the future we implement per-tenant roles, this step would differ, but for now, this approach aligns with the blueprint’s simplified role strategy.)
* **Initial Feature Flag Setup:** After structural setup, populate the new account’s feature flags in `public.account_features`. This mirrors what the seed process does, but now for this specific account:

  * Look up all features from `public.features`. For each feature, determine if it should be enabled or disabled for this account’s plan.
  * Since new tenants default to Free plan, set premium features off. For example, for features like `CUSTOM_DOMAIN` or `WHITE_LABEL` that are Pro-only, insert `account_id = new_account_id, feature_key = 'CUSTOM_DOMAIN', enabled = false` (because on Free it’s not allowed). If there are features that Free does have (not likely if we treat Free as base and Pro as superset), those would be true. In practice, it might be that all toggled features are things Free lacks, so most inserts are `enabled=false` for a Free account. Regardless, insert a row for each feature key to explicitly set the state.
  * Use a transaction or careful error handling: if the account\_features inserts fail (perhaps because a feature row exists, though it shouldn’t for a new account), handle or log it.
  * If a `--plan` was specified and the account is created as Pro (or another plan), then mark accordingly – e.g., if Pro, set those premium features true. Essentially, align the `account_features` entries with the plan’s entitlements at creation. (Also update the `accounts.plan_id` if non-default plan used.)
* **Owner User Membership:** Finally, associate a user with this new tenant as the owner:

  * If the CLI was given an existing `user_id` or `user_email` for the owner, use it. If an email was provided but no such user exists in `auth.users`, you might create that user (e.g., invite them or create with a temp password). If no user info was provided, decide on a default action: one approach is to attach the current platform admin as the owner (not typically desired for a real tenant), or simply create the account with no members and instruct that an invite must be sent separately. Ideally, require an owner to be specified to avoid orphaned accounts.
  * Insert into `public.account_members` a row with `account_id = new_account_id, user_id = (owner’s uuid), role = 'Owner'`. This gives that user full rights to the new account. This will also ensure that when the user logs in, the Supabase JWT function will include this account and role for them.
  * If the user was just created (and perhaps an invitation email needs to be sent), handle that outside or note it. If using an existing user who already belonged to another account, note that now they are multi-tenanted – the JWT may only pick up one account by default (MVP limitation), so in MVP that user might not automatically switch to this new account without some mechanism. (This scenario is uncommon unless an internal admin is being attached.)
* **Post-Creation Verification:** After creating a tenant, perform checks to ensure everything is set up:

  * Query `public.accounts` for the new account name/ID to confirm insertion. Query `public.account_members` to ensure the owner membership is in place.
  * Check that the new schema exists by querying `pg_namespace` or using a DB inspection. Ensure tables like `<schema>.sites` exist. Optionally, insert a quick test row (e.g., create a site via the API or direct insert as that user) to verify that data goes into the correct schema and that RLS allows it.
  * Using the Supabase JS client with the owner user’s JWT, try to read from one of the tenant tables (e.g., fetch from `<schema>.sites`) to ensure the owner can access their data. Also verify that another user from a different account cannot access this new schema’s data (attempt a cross-tenant query with a different JWT should fail or return nothing, demonstrating isolation).
  * If any part of the setup failed (e.g., one table creation failed), the CLI should ideally rollback what it did (for instance, drop the schema and remove the account row if we encountered a fatal error after creating some things). Implement transactions for the account insert + feature inserts, and try/catch around the DDL statements. Because some DDL (create schema) can’t run inside a transaction on the same connection easily, handle errors carefully and clean up resources if needed.
* **Logging and Output:** Have the CLI output a summary: e.g., “Tenant `<name>` (ID: `<uuid>`) created with schema `<schema_name>`. Owner user: `<user_email>`.” If appropriate, display next steps (for example, if no owner was provided: “No user attached to this account – you will need to invite a user to this account to use it.”).
* **Integration with Signup Flow:** Note that internally, the user signup flow triggers this same procedure. Ensure that the logic here can be reused or invoked programmatically. For instance, the CLI command might call an underlying module or function `createTenant(name, userId)` that the Next.js API route can also call when a new user signs up. This avoids duplication. The blueprint explicitly mentions using the CLI internally for provisioning a new account on sign-up.
* **Testing:** Test the `create-tenant` command thoroughly:

  * Create a new tenant via CLI with a test user as owner. Then log in as that user (or use their token) to confirm they see their account and can create content. Check that the `account_features` for that account correctly reflect the Free plan defaults (e.g., try using a feature that should be off and see it blocked).
  * Test creating a tenant with a name that would slugify to something existing to ensure the uniqueness logic works (e.g., create "TestCo", then "Test Co" and see the second gets a different schema name).
  * Test without providing `--owner`: the command should still create the account and schema. Then separately add a user to it (manually or via another CLI command) and confirm everything links up.
  * Verify that if the user already existed and had no account, they now can log in and get the new account in their JWT. If the user had another account, in MVP the JWT might not automatically include the new one (because Supabase custom claims might pick the first account). In such a case, consider how to handle multi-account users – possibly document that multi-account support is limited and the user might need to switch accounts via an admin tool.
  * Confirm that RLS policies and triggers are effectively isolating the new tenant. For instance, attempt a SQL query as an `authenticated` role user from another account selecting data from the new schema (it should return 0 rows or permission denied).
* **Maintenance:** Each time we evolve the schema (new tables or columns in tenant schemas), update the template or logic used in `create-tenant` accordingly. Also, if new features or plans are added, ensure `create-tenant` sets those up for new accounts (e.g., if an Enterprise plan is added, handle assigning features if a tenant is created with that plan). The goal is that this command completely automates what an initial sign-up would need, matching the latest blueprint-defined schema and defaults, without any manual tweaks.

**Sources:**

* Ozza Platform Blueprint v5 – Database schema, multi-tenancy, and seeding guidelines.
